summary(mydata1)
# a simple multiple regression trial
model = lm(log(price)~log(summer)+log(har)+log(win)+log(sep)+log(age),data=mydata1)
# log-log regression results
summary(model)
# Stepwise Model
stepwise_model <- step(model, direction="both") #
# Display the summary of the stepwise model
summary(stepwise_model)
# Finalize Model
final_model <- lm(formula = log(price) ~ log(summer) + log(har) + log(win) + log(age) + log(winter) * log(summer), data = mydata1)
summary(final_model)
# Finalize Model
final_model <- lm(formula = log(price) ~ log(summer) + log(har) + log(win) + log(age) + log(winter) * log(summer), data = mydata1)
summary(final_model)
# Finalize Model
final_model <- lm(formula = log(price) ~ log(summer) + log(har) + log(win) + log(age) + log(win) * log(summer), data = mydata1)
summary(final_model)
# Finalize Model
final_model <- lm(formula = log(price) ~ log(summer) + log(har) + log(win) + log(age), data = mydata1)
summary(final_model)
hist(mydata1$win)
boxplot(mydata1$win)
boxplot(mydata1$log(win))
boxplot(log(mydata1$win))
boxplot(mydata1$win)
View(mydata2)
boxplot(mydata1$price)
# EDA
par(mfrow = c(3, 3))
for (variable in names(mydata1)) {
boxplot(mydata1[[variable]], main = paste("Histogram of", variable), xlab = variable, col ="red")
}
par(mfrow = c(1, 1))
hist(mydata1$sep)
boxplot(mydata1$price)
boxplot(mydata1$price, main = "Boxplot Title", ylab = "Y-axis Label", col = "lightblue")
points(jitter(rep(1, length(mydata1$price))), mydata1$price, col = "red", pch = 20)
boxplot(mydata1$price, main = "Boxplot Title", ylab = "Y-axis Label", col = "lightblue")
boxplot(mydata1$price, main = "Histogram of Price", ylab = "Price", col = "lightblue")
# EDA
par(mfrow = c(3, 3))
for (variable in names(mydata1)) {
hist(mydata1[[variable]], main = paste("Histogram of all variables", variable), xlab = variable, col ="red")
}
boxplot(mydata1[[variable]], main = paste("Boxplots of ", variable), xlab = variable, col ="red")
for (variable in names(mydata1)) {
boxplot(mydata1[[variable]], main = paste("Boxplots of ", variable), xlab = variable, col ="red")
}
for (variable in names(mydata1)) {
hist(mydata1[[variable]], main = paste("Histogram of all variables", variable), xlab = variable, col ="red")
}
# EDA
par(mfrow = c(3, 3))
for (variable in names(mydata1)) {
hist(mydata1[[variable]], main = paste("Histogram of", variable), xlab = variable, col ="red")
}
library(ggplot2)
library(GGally)
ggpairs(mydata1, title = "Correlation Matrix")
mydata1 <- subset(mydata1, price != 100)
# summary statistics
summary(mydata1)
# a simple multiple regression trial
model = lm(log(price)~log(summer)+log(har)+log(win)+log(sep)+log(age),data=mydata1)
# log-log regression results
summary(model)
# Normality Test by QQ and Shapiro-Wilk
residuals_model <- residuals(model)
qqnorm(residuals_model)
qqline(residuals_model, col="red")
shapiro.test(residuals_model)
# Serial Correlation by DW
library(car)
durbinWatsonTest(model)
# Heteroskedasticity by fitted vs residuals
library(lmtest)
plot(fitted(model), residuals_model)
abline(h=0, col="red")
bptest(model)
# Stepwise Model
stepwise_model <- step(model, direction="both") #
# Display the summary of the stepwise model
summary(stepwise_model)
# a simple multiple regression trial
model = lm(log(price)~log(summer)+log(har)+log(win)+log(sep)+log(age),data=mydata1)
# log-log regression results
summary(model)
# Simple multiple regression with all variables concerned and log-transformation
initial_model = lm(log(price)~log(summer)+log(har)+log(win)+log(sep)+log(age),data=mydata1)
summary(initial_model)
# Using Stepwise methods ta choose the best model
stepwise_model <- step(initial_model, direction="both")
summary(stepwise_model)
library(car)
library(lmtest)
# Normality Test by QQ and Shapiro-Wilk
residuals_model <- residuals(stepwise_model)
qqnorm(residuals_model)
qqline(residuals_model, col="red")
shapiro.test(residuals_model)
# Normality Test by QQ and Shapiro-Wilk
par(mfrow = c(1, 1))
residuals_model <- residuals(stepwise_model)
qqnorm(residuals_model)
qqline(residuals_model, col="red")
shapiro.test(residuals_model)
# Serial Correlation by DW
durbinWatsonTest(stepwise_model)
bptest(stepwise_model)
# residual diagnostics
par(mfrow = c(2,2), ask=F)
plot(model)
# Heteroskedasticity by fitted vs residuals
plot(fitted(stepwise_model), residuals_model)
abline(h=0, col="red")
# Heteroskedasticity by fitted vs residuals
par(mfrow = c(1, 1))
plot(fitted(stepwise_model), residuals_model)
abline(h=0, col="red")
# inputs: regression model and independent variables
pred = predict(model,mydata2,interval="prediction")
pred
exp(pred)
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction", level = 0.95)
exp(pred)
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction", level = 0.95, interval = 'prediction')
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction", level = 0.95, interval = c('prediction')
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction", level = 0.95, interval = c('prediction')
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction", level = 0.95, interval = c('prediction'))
exp(pred)
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction", level = 0.95, interval = 'prediction))
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction", level = 0.95, interval = 'prediction')
exp(pred)
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction")
exp(pred)
# out-of-sample prediction line
plot(mydata2[,1],pred[,1],type="l",xlab="year",ylab="log_price",ylim=c(0,8),
col=2, lty=2, lwd=2,main="out-of-sample prediction line")
lines(mydata2[,1],pred[,2],col=3, lty=3, lwd=3)
lines(mydata2[,1],pred[,3],col=4, lty=4, lwd=4)
legend("topleft", c("fit","lwr","upr"),col=2:4, lty=2:4, lwd=2:4, bty="n")
# exponentiate back the predicited price
price_hat = exp(pred[,1])
plot(mydata2[,1],price_hat,type="l",xlab="year",ylab="predicted price",ylim=c(0,50),
col=2, lty=2, lwd=2,main="out-of-sample prediction line")
lines(mydata2[,1],pred[,2],col=3, lty=3, lwd=3)
pred <- exp(pred)
# out-of-sample prediction line
plot(mydata2[,1],pred[,1],type="l",xlab="year",ylab="log_price",ylim=c(0,8),
col=2, lty=2, lwd=2,main="out-of-sample prediction line")
# out-of-sample prediction line
plot(mydata2[,1],pred[,1],type="l",xlab="year",ylab="price",ylim=c(0,100),
col=2, lty=2, lwd=2,main="out-of-sample prediction line")
lines(mydata2[,1],pred[,2],col=3, lty=3, lwd=3)
lines(mydata2[,1],pred[,3],col=4, lty=4, lwd=4)
legend("topleft", c("fit","lwr","upr"),col=2:4, lty=2:4, lwd=2:4, bty="n")
highlighted_years <- c(1986)
highlighted_points <- which(mydata2[,1] %in% highlighted_years)
points(mydata2[highlighted_points,1], pred[highlighted_points,1], pch=16, col="gold", cex=1.5)
# out-of-sample prediction line
plot(mydata2[,1],pred[,1],type="l",xlab="year",ylab="price",ylim=c(0,100),
col=2, lty=2, lwd=2,main="out-of-sample prediction line")
lines(mydata2[,1],pred[,2],col=3, lty=3, lwd=3)
lines(mydata2[,1],pred[,3],col=4, lty=4, lwd=4)
legend("topleft", c("fit","lwr","upr"),col=2:4, lty=2:4, lwd=2:4, bty="n")
highlighted_years <- c(1986) # Hightlight the year 1986
highlighted_points <- which(mydata2[,1] %in% highlighted_years)
points(mydata2[highlighted_points,1], pred[highlighted_points,1], pch=16, col="gold", cex=1.5)
# predict new observation, for year 2017
newdata = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = 2 )
exp(predict(model, newdata))
summary(model)
# Model
model <- lm(formula = log(price) ~ log(summer) + log(har) + log(win) + log(age), data = mydata1)
summary(model)
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction")
pred <- exp(pred)
pred
summary(model)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 0)
# predict new observation, for year 2017, 2018, 2019
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = 2 )
newdata_2018 = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = 1)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
new_prediction <- exp(predict(model,c(newdata_2017,newdata_2018,newdata_2019)))
new_prediction
new_prediction <- exp(predict(model,newdata_2017))
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 0.01)
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
# predict new observation, for year 2017, 2018, 2019
current_year <- as.numeric(format(Sys.Date(), "%Y")) # Get Current Year
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = current_year - 2017)
newdata_2018 = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = current_year - 2018)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = current_year - 2019)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = (current_year - 2017))
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
newdata_2018 = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = current_year - 2018)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = current_year - 2019)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
summary(model)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 50)
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
# Data
mydata1 = read.table("http://faculty.chicagobooth.edu/nicholas.polson/teaching/41000/bordeaux.txt",header=T)
mydata2 = read.table("http://faculty.chicagobooth.edu/nicholas.polson/teaching/41000/bordeauxp.txt",header=T)
# Remove the outlier
mydata1 <- subset(mydata1, price != 100)
# Model
model <- lm(formula = log(price) ~ log(summer) + log(har) + log(win) + log(age), data = mydata1)
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = current_year - 2017)
newdata_2018 = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = current_year - 2018)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = current_year - 2019)
prediction_2017 <- exp(predict(model,newdata_2017))
# predict new observation, for year 2017, 2018, 2019
current_year <- as.numeric(format(Sys.Date(), "%Y")) # Get Current Year
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = current_year - 2017)
newdata_2018 = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = current_year - 2018)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = current_year - 2019)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
# predict new observation, for year 2017
newdata = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = 2 )
exp(predict(model, newdata))
# predict new observation, for year 2018
newdata = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = 1)
exp(predict(model, newdata))
source("~/.active-rstudio-document")
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = current_year - 2017)
newdata_2018 = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = current_year - 2018)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = current_year - 2019)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = 6)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
# predict new observation, for year 2017
newdata = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = 6 )
exp(predict(model, newdata))
scatterplot(mydata1$year,mydata1$price)
scatterplot(mydata1$age,mydata1$age)
scatterplot(mydata1$price,mydata1$age)
scatterplot(mydata1$price,mydata1$age)
# out-of-sample prediction line
plot(mydata2[,1],pred[,1],type="l",xlab="year",ylab="price",ylim=c(0,100),
col=2, lty=2, lwd=2,main="out-of-sample prediction line")
lines(mydata2[,1],pred[,2],col=3, lty=3, lwd=3)
lines(mydata2[,1],pred[,3],col=4, lty=4, lwd=4)
legend("topleft", c("fit","lwr","upr"),col=2:4, lty=2:4, lwd=2:4, bty="n")
highlighted_years <- c(1986) # Hightlight the year 1986
highlighted_points <- which(mydata2[,1] %in% highlighted_years)
points(mydata2[highlighted_points,1], pred[highlighted_points,1], pch=16, col="gold", cex=1.5)
View(mydata2)
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
scatterplot(mydata1$price,mydata1$age)
summary(model)
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = current_year - 2019)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = current_year - 2017)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
exp(predict(model,data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 1)))
exp(predict(model,data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 2)))
exp(predict(model,data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 3)))
exp(predict(model,data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 4)))
exp(predict(model,data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 5)))
exp(predict(model,data.frame(win = 550, summer = 21, sep = 19, har = 110, age = 10)))
scatterplot(mydata1$price,mydata1$win)
summary(model)
scatterplot(mydata1$price,mydata1$har)
summary(model)
scatterplot(mydata1$sep,mydata1$har)
summary(model)
scatterplot(mydata1$sep,mydata1$price)
summary(model)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
# predict new observation, for year 2017
newdata = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = 2 )
exp(predict(model, newdata))
# Dependency
library(ggplot2)
library(GGally)
library(car)
# Dependency
library(ggplot2)
library(GGally)
library(car)
library(lmtest)
# Data
mydata1 = read.table("http://faculty.chicagobooth.edu/nicholas.polson/teaching/41000/bordeaux.txt",header=T)
mydata2 = read.table("http://faculty.chicagobooth.edu/nicholas.polson/teaching/41000/bordeauxp.txt",header=T)
# Boxplot
par(mfrow = c(3, 3))
for (variable in names(mydata1)) {
boxplot(mydata1[[variable]], main = paste("Boxplots of", variable), xlab = variable, col ="red")
}
par(mfrow = c(1, 1))
hist(mydata1$price)
View(mydata1)
# Boxplot
par(mfrow = c(3, 3))
for (variable in names(mydata1)) {
boxplot(mydata1[[variable]], main = paste("Boxplots of", variable), xlab = variable, col ="red")
}
par(mfrow = c(1, 1))
boxplot(mydata1$price)
# Histogram
par(mfrow = c(3, 3))
for (variable in names(mydata1)) {
hist(mydata1[[variable]], main = paste("Histogram of", variable), xlab = variable, col ="red")
}
# Correlation Matrix
ggpairs(mydata1, title = "Correlation Matrix")
# Remove the outlier
mydata1 <- subset(mydata1, price != 100)
# Simple multiple regression with all variables concerned and log-transformation
initial_model = lm(log(price)~log(summer)+log(har)+log(win)+log(sep)+log(age),data=mydata1)
summary(initial_model)
# Using Stepwise methods ta choose the best model
stepwise_model <- step(initial_model, direction="both")
summary(stepwise_model)
summary(initial_model)
# Normality Test by QQ and Shapiro-Wilk
par(mfrow = c(1, 1))
residuals_model <- residuals(stepwise_model)
qqnorm(residuals_model)
qqline(residuals_model, col="red")
shapiro.test(residuals_model)
# Serial Correlation by DW
par(mfrow = c(1, 1))
durbinWatsonTest(stepwise_model)
# Heteroskedasticity by fitted vs residuals
par(mfrow = c(1, 1))
plot(fitted(stepwise_model), residuals_model)
abline(h=0, col="red")
bptest(stepwise_model)
# Model
model <- lm(formula = log(price) ~ log(summer) + log(har) + log(win) + log(age), data = mydata1)
# Predict the year from 1981 to 1991
pred = predict(model,mydata2,interval="prediction")
pred <- exp(pred)
pred
# out-of-sample prediction line
plot(mydata2[,1],pred[,1],type="l",xlab="year",ylab="price",ylim=c(0,100),
col=2, lty=2, lwd=2,main="out-of-sample prediction line")
lines(mydata2[,1],pred[,2],col=3, lty=3, lwd=3)
lines(mydata2[,1],pred[,3],col=4, lty=4, lwd=4)
legend("topleft", c("fit","lwr","upr"),col=2:4, lty=2:4, lwd=2:4, bty="n")
highlighted_years <- c(1986) # Hightlight the year 1986
highlighted_points <- which(mydata2[,1] %in% highlighted_years)
points(mydata2[highlighted_points,1], pred[highlighted_points,1], pch=16, col="gold", cex=1.5)
# predict new observation, for year 2017, 2018, 2019
current_year <- as.numeric(format(Sys.Date(), "%Y")) # Get Current Year
newdata_2017 = data.frame(win = 521.07, summer = 19, sep = 19, har = 86.2, age = current_year - 2017)
newdata_2018 = data.frame(win = 691, summer = 20, sep = 22, har = 84.9, age = current_year - 2018)
newdata_2019 = data.frame(win = 550, summer = 21, sep = 19, har = 110, age = current_year - 2019)
prediction_2017 <- exp(predict(model,newdata_2017))
prediction_2017
prediction_2018 <- exp(predict(model,newdata_2018))
prediction_2018
prediction_2019 <- exp(predict(model,newdata_2019))
prediction_2019
View(mydata2)
View(mydata1)
setwd("~/Documents/Uchicago/Winter 2024/Linear & Nonlinear Models/Week 3")
load("1030.lecture-mle.RData")
load("1030.lecture-mle.RData")
View(dta)
#Define y and n
y <- dta$binomial$y
n <- dta$binomial$n
#Assuming we can create p with seq(0.01, .99, by = 0.01)
p <- seq(.01, .99, by=.01)
#Calculate the log likelihood for various values of p
loglike <- sum(log(choose(n, y))) + log(p)*sum(y) + log(1-p)*sum(n-y)
#Calculate the derivative of the log likelihood for various values of p
dloglike <- sum(y)/p - sum(n-y)/(1-p)
#MLE esimtate for p
p_hat <- sum(y)/sum(n)
#PLOT THE FIRST SET
plot(x = 1:length(y), y = y, type = 'l', col = 'red', xlab = '1:m')
# Load 'y' and 'n' from the binomial component of the dataset 'dta'
y <- dta$binomial$y
n <- dta$binomial$n
# Create a sequence 'p' ranging from 0.01 to 0.99 with increments of 0.01
p <- seq(0.01, 0.99, by = 0.01)
# Compute the log-likelihood for a range of 'p' values
loglike <- sum(log(choose(n, y))) + sum(y) * log(p) + sum(n - y) * log(1 - p)
# Determine the gradient of the log-likelihood with respect to 'p'
dloglike <- sum(y) / p - sum(n - y) / (1 - p)
# Estimate the maximum likelihood estimator (MLE) for 'p'
p_hat <- sum(y) / sum(n)
# Generate a line plot with the 'y' values over the sequence '1:m' with a red line
plot(x = 1:length(y), y = y, type = 'l', col = 'red', xlab = '1:m')
dta <- load("1030.lecture-mle.RData")
# Load 'y' and 'n' from the binomial component of the dataset 'dta'
y <- dta$binomial$y
# Load 'y' and 'n' from the binomial component of the dataset 'dta'
y <- dta$binomial$y
load("1030.lecture-mle.RData")
# Load 'y' and 'n' from the binomial component of the dataset 'dta'
y <- dta$binomial$y
n <- dta$binomial$n
# Create a sequence 'p' ranging from 0.01 to 0.99 with increments of 0.01
p <- seq(0.01, 0.99, by = 0.01)
# Compute the log-likelihood for a range of 'p' values
loglike <- sum(log(choose(n, y))) + sum(y) * log(p) + sum(n - y) * log(1 - p)
# Determine the gradient of the log-likelihood with respect to 'p'
dloglike <- sum(y) / p - sum(n - y) / (1 - p)
# Estimate the maximum likelihood estimator (MLE) for 'p'
p_hat <- sum(y) / sum(n)
# Generate a line plot with the 'y' values over the sequence '1:m' with a red line
plot(x = 1:length(y), y = y, type = 'l', col = 'red', xlab = '1:m')
# Set up the graphics layout to display two plots side by side
par(mfrow = c(1, 2))
# Create a line plot of the log-likelihood values over the range of 'p' probabilities,
# and add a vertical line to indicate the position of the MLE for 'p'
plot(x = p, y = loglike, type = 'l', col = 'red', xlab = 'Probability')
abline(v = p_hat, col = 'black')
# Create a line plot of the derivative of the log-likelihood values over the range of 'p' probabilities,
# and add a vertical line to indicate the position of the MLE for 'p'
plot(x = p, y = dloglike, type = 'l', col = 'red', xlab = 'Probability')
abline(v = p_hat, col = 'black')
# Load 'y' from the Poisson component of the dataset 'dta'
y <- dta$poisson$y
# Generate a sequence 'lb' representing different values of lambda, from 0.5 to 12 in 0.1 increments
lambda_sequence <- seq(0.5, 12, by = 0.1)
# Compute the log-likelihood for the range of lambda values
log_likelihood <- sum(y) * log(lambda_sequence) - length(y) * lambda_sequence - sum(log(factorial(y)))
# Determine the gradient of the log-likelihood with respect to lambda values
derivative_log_likelihood <- sum(y) / lambda_sequence - length(y)
# Estimate the maximum likelihood estimator (MLE) for lambda
lambda_hat <- mean(y)
# Set up the plotting area to display two plots side by side
par(mfrow = c(1, 2))
# Plot 'y' as a histogram with vertical lines, colored red, to indicate the MLE of lambda
plot(y, type = "h", col = "red")
abline(h = lambda_hat)
# Create a histogram of 'y' and overlay a vertical red line indicating the MLE of lambda
hist(y)
abline(v = lambda_hat, col = "red")
# Create a plot displaying the log-likelihood against the lambda values with a line plot
# Highlight the MLE for lambda with a vertical line
plot(x = lb, y = log_likelihood, type = 'l', col = 'red', xlab = 'Lambda')
# Create a plot displaying the log-likelihood against the lambda values with a line plot
# Highlight the MLE for lambda with a vertical line
plot(x = lb, y = log_likelihood, type = 'l', col = 'red', xlab = 'Lambda')
# Create a plot displaying the log-likelihood against the lambda values with a line plot
# Highlight the MLE for lambda with a vertical line
plot(x = lambda_sequence, y = log_likelihood, type = 'l', col = 'red', xlab = 'Lambda')
abline(v = lambda_hat, col = 'black')
# Create a plot showing the derivative of the log-likelihood with respect to lambda
# Indicate the MLE for lambda with a vertical line
plot(x = lambda_sequence, y = derivative_log_likelihood, type = 'l', col = 'red', xlab = 'Lambda')
abline(v = lambda_hat, col = 'black')
# Set up the plotting area to display two plots side by side
par(mfrow = c(1, 2))
# Create a plot displaying the log-likelihood against the lambda values with a line plot
# Highlight the MLE for lambda with a vertical line
plot(x = lambda_sequence, y = log_likelihood, type = 'l', col = 'red', xlab = 'Lambda')
abline(v = lambda_hat, col = 'black')
# Create a plot showing the derivative of the log-likelihood with respect to lambda
# Indicate the MLE for lambda with a vertical line
plot(x = lambda_sequence, y = derivative_log_likelihood, type = 'l', col = 'red', xlab = 'Lambda')
abline(v = lambda_hat, col = 'black')
